{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnif5iXTmTck"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "Let's build a replication of word2vec from scratch!\n",
        "\n",
        "Word2Vec is build off a few fundamental ideas which are important to understand\n",
        "- Distributional semantics (\"a word is known by the company it keeps\")\n",
        "- We can take neural network weights and directly use them as word vectors"
      ],
      "metadata": {
        "id": "7XEo863LmV5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two versions of Word2Vec\n",
        "- Skip gram: predict word from its context\n",
        "- CBOW: predict context from a word\n",
        "\n",
        "Each of them is built by one-hot encoding each word and using a sliding window over corpus of text- training a 1-hidden-layer NN to predict (using tasks from above).\n",
        "\n",
        "We can then use the weights as vectors (since we've encoded meaning that should roughly correspond to their distributional semantics- e.g. blue should have similar vector to green)"
      ],
      "metadata": {
        "id": "VORULYObmtuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data + Pre-processing\n",
        "We first need to actually load some data for this task- and preprocess/tokenize it"
      ],
      "metadata": {
        "id": "2I2D50rknOeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "### Step 2: Define Dataset Class\n",
        "\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, path\n",
        "                 , context_size=2, transform=None):\n",
        "        \"\"\"\n",
        "        path: Path to the text file.\n",
        "        context_size: Number of words on each side of the target word to include in the context.\n",
        "        transform: Any transformation to apply to the list of words.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.context_size = context_size\n",
        "\n",
        "        # Open, read, and preprocess the text\n",
        "        with open(path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read().lower()  # Convert to lowercase\n",
        "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "            self.words = word_tokenize(text)  # Tokenize into words\n",
        "\n",
        "        if self.transform:\n",
        "            self.words = self.transform(self.words)\n",
        "\n",
        "        self.vocab = self.build_vocab(self.words)\n",
        "        self.word_indices = [self.vocab[word] for word in self.words]\n",
        "\n",
        "        # Prepare training instances (context, target)\n",
        "        self.data = []\n",
        "        print(f\"Iterating from {context_size} to {len(self.word_indices) - context_size}\")\n",
        "        print(f\"First block will be {-context_size + context_size} to {context_size + context_size + 1}\")\n",
        "\n",
        "\n",
        "        for i in range(context_size, len(self.word_indices) - context_size):\n",
        "            context = torch.tensor([self.word_indices[i + j] for j in range(-context_size, context_size + 1) if j != 0])\n",
        "            target = torch.tensor([self.word_indices[i]])\n",
        "            [i]\n",
        "            self.data.append((context, target))\n",
        "\n",
        "    def build_vocab(self, words):\n",
        "        \"\"\"\n",
        "        Build vocabulary: Mapping of word -> unique index\n",
        "        \"\"\"\n",
        "        word_counts = Counter(words)\n",
        "        # Start indexing from 1; 0 will be used for padding if necessary\n",
        "        vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.items())}\n",
        "        return vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjMSiKOgmtGb",
        "outputId": "f532cb44-a10b-473d-b724-b92650430002"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "dataset = Word2VecDataset(path='shake.txt', context_size=2, transform=remove_stopwords)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Vocabulary size: {dataset.get_vocab_size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kSTKBwcmqb6",
        "outputId": "552b7d13-517d-4126-8ef5-1b46f98a2b0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterating from 2 to 481446\n",
            "First block will be 0 to 5\n",
            "Dataset size: 481444\n",
            "Vocabulary size: 28207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size):\n",
        "    super(SkipGramModel, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.linear_layer= nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "  def forward(self, word_idx):\n",
        "    embedding = self.embeddings(word_idx)\n",
        "    output = self.linear_layer(embedding)\n",
        "    log_probs = F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "    # self.\n",
        ""
      ],
      "metadata": {
        "id": "5xq2GQavqkTl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqFWJckcq5w_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vocab_size = dataset.get_vocab_size()\n",
        "embedding_size = 1536\n",
        "model = SkipGramModel(vocab_size, embedding_size)"
      ],
      "metadata": {
        "id": "xvSl086foW73"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ],
      "metadata": {
        "id": "SteYndoMpmyP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "device = torch.device(\"cuda\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for (context, target) in dataloader:\n",
        "\n",
        "        # context and target initially [batch_size, context_size], [batch_size, 1]\n",
        "        batch_size, context_size = context.size()\n",
        "\n",
        "        context, target = context.to(device), target.to(device).squeeze(-1)\n",
        "\n",
        "        # Flatten context and repeat target to match each context word\n",
        "        context = context.view(-1)  # Flatten to [batch_size * context_size]\n",
        "        target = target.repeat_interleave(context_size)  # Repeat each target word `context_size` times\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model.to(device)\n",
        "        # Model expects flattened context now\n",
        "        output = model(context)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"loss {loss}\")\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "tC-aNuSNyB4B",
        "outputId": "4ba6b47e-aa32-4c00-90db-1d17deeb088f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-33d6afe706e1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Flatten context and repeat target to match each context word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for (context, target) in dataloader:\n",
        "        context, target = context.to(device), target.to(device)\n",
        "        print(context.shape, target.shape)\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(context)\n",
        "        loss = loss_fn(output, target.squeeze(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss over an epoch\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    # loss_values.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "zmvBUVvjpwrr",
        "outputId": "c679498d-3ab5-450f-9f8a-f26a4add2cc4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 4]) torch.Size([128, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected target size [128, 28207], got [128]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-16b891f15ec2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2729\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [128, 28207], got [128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnG4-aONpzZj",
        "outputId": "9af14f0f-9ef4-4015-9a79-bc10549f78e4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[tensor([ 6452,   513,   627,  9154,  3572,   567,  1116,   833,   550,  1508,\n",
              "           2146,  9070,   149,  4908,  5436,  6047,  4588,  1185,  4740,   833,\n",
              "           3716,  8105,  4812,   555, 14334,   555,  2045,  5843,  1316,   957,\n",
              "            443,  2298,  1366, 11341,  3569,  2305,   640,   613, 14344, 21650,\n",
              "            488, 11810,  1157,   764,   968,  3451, 14845, 12020,   398,  1634,\n",
              "           4725,   822,  1478,   537,  1765,  3595, 16413,  1282,  7628,   846,\n",
              "           6460, 10592,   115,   537]),\n",
              "  tensor([12403,  7543,  2650,   442,   159,   970,   136,  1418,  1156,  3106,\n",
              "           5809,  9071,   150,  3126,  7480, 13033,  3588,  2038,  1069,  2150,\n",
              "          13353,   980,  1083,   626,  2184,  4478,  2058,  5576,  1273,  4509,\n",
              "           3568,  3126, 17251,    55,   125,   623,  1107,  2189,   115,  2045,\n",
              "           3127,  3061, 14889,  1503,  3787,   282, 16239,   654,  1840, 10764,\n",
              "             44,   817,  1299,  3480,   956, 14346,    74,  2035,  3672,  3080,\n",
              "           1105,  4447,  3991,   390]),\n",
              "  tensor([  695, 20463, 19091, 18293,  3578,  2441,   660,  2219,  1947,   495,\n",
              "          13586,  8768,   151,   153,   746,  1185,   995,  4822, 19053,  2241,\n",
              "          12069,   608,  1073,  1316,   699,  1405,  2069, 17900,  3603,  4510,\n",
              "           3595,   496,  1160,  2088,   427, 10183,  6632,  2249,  3616,  2058,\n",
              "            641,  1612, 18978,  1508,  3655, 21649,  2146,   571,  1316,   496,\n",
              "          17238,   822,  3426,  7159,   214, 13374, 13293,  1146,  6458,  3407,\n",
              "           3417,  1901, 15136,  3997]),\n",
              "  tensor([21057, 21008, 20477, 18294,  3572, 14264,  1174,  1231,   970,   537,\n",
              "           1499,  8763,   152,   583,  1231, 20585,   775,  5330,  6137,   223,\n",
              "           4879,   657,  1465,  2446,   465,  4935,  5057, 12727,  2146,  4509,\n",
              "            687,  1151, 14334,   113,  1644,  5682,   389,  1418, 15683,   537,\n",
              "           3127, 17522,   290,  1310,   487,  2373, 14313, 12415,   966,  6341,\n",
              "           4696,   973,   537,   575,  2169,   153,  3411,  5716,  7647,  1647,\n",
              "          13951,   113,  4487,  3577])],\n",
              " tensor([ 3603, 12102,  1668, 17932,  2035,  7605,  1316,  4975,  9809,  3107,\n",
              "          2954,   223,     3, 11198,  1049,  1019,  1256,  3187, 19052,  7630,\n",
              "            74,  3299,   990,  1053,  1363,  2123,  1772, 17899,  3832,  1435,\n",
              "         10861,   813,  2196,   502,   414,   211,  3392,   771, 15271,  4509,\n",
              "           641, 17521,   956,  1240,   465,  3064,  7498,   290,   731, 10764,\n",
              "           719,  1166,  2184,   833,   537, 11582,  2146,  4041,  3590,  9488,\n",
              "          1018,  3679,  3710,  3996])]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1F2p8ZmqDWT",
        "outputId": "d351493a-9c51-42b4-e144-98780de96ec7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([  113,   142,  9799,  1341,  1566,  3568,  6087,  3926,  2652,   465,\n",
              "          9482,   829,   627, 13324,    55, 10434,   726,  6060,  4975, 16067,\n",
              "          4041,  1895,  3595,  7037,  3920,  2146,   567,  4213,   838,  4975,\n",
              "           735,   567,  1829,   646,   537,   578,   567,  5383,   943,   403,\n",
              "          4590,  1262,  1116, 14906,  5041,  2865,  7639,    47,  9628,  5725,\n",
              "            38,  2028,  3879,  7838,  2373,   223,   487,  5823,  3402,  6846,\n",
              "          1240,  2403,  8537,  1865]),\n",
              " tensor([12941, 13317,   775, 21162,  2146,  1316,  6777,   973,  1185,  3521,\n",
              "           590,   150,  2211,    91,  1203,   575,   537,   833,  1561,  4988,\n",
              "          1733,  6441, 13300, 14845,   537,   297,  6074,  1008, 18651,  1698,\n",
              "          6536,  4135,   282,  3293,   950,   740,   661,  6194,  4609,  3407,\n",
              "           692,  1174,  1004,   911,  6288,   683,   699,    48,  3804,  6258,\n",
              "           867,  4513,  6246,   654,  5739,   290,  5436,  2146,  2146,  1924,\n",
              "           177, 10575,   510,  2373]),\n",
              " tensor([  444,  1727,  9489,   170,  2239,   442,    38,  5756,  3569, 21216,\n",
              "          1282,   735,   920,   374,  8763,  7462,  1829,  1019,   114,  1970,\n",
              "          7733,  1231, 13317,  1316,  7793,  2161,  1231,   830,  3402, 14011,\n",
              "          1509,  5716,   564,  3788,   842,  4043,   660, 15271,   929, 17237,\n",
              "           913,   125,  3151, 15626,  2174,  7383,   846,    50,  4977,  6259,\n",
              "           537,   597,  1375,  1852,   502,  5736,  3063, 12364,  1174,  6847,\n",
              "          2516,  2900,  1505,  3671]),\n",
              " tensor([  794,  4645,  9800,   621,  3836, 17932,  1745,  1655, 14333,  3082,\n",
              "           911,  1478,   537,  1641,  3726,  1221,  4222,  1759,  3987,   660,\n",
              "           621,   943,  2568,   699,  3668,  2267,   580, 13383,   481,  2979,\n",
              "          2851,  4725,   496,  4923, 19569,   584,  1240,   150,  3595,   223,\n",
              "           749,  4130,   113,  6265,  2885,  9772,   150,    51,   465,   282,\n",
              "           859,   660,   534,  2119,  1718,  1073,  1245,  5670, 10777,  1418,\n",
              "           787,   481,    55, 21157])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLaIF8RfqVYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}